# Poorman's AR-DiT TTS ğŸ“¢

> **Keywords**: ARDiT, AR-DiT, Autoregressive Diffusion Transformer, TTS, Text-to-Speech, Mel-Spectrogram

A **resource-friendly** Text-to-Speech system inspired by AR-DiT (ARDiT), combining an autoregressive Transformer (Qwen3 LLM) with a diffusion model architecture. It generates Mel spectrograms through a diffusion process, then converts them to audio via a Vocoder.

**âœ¨ Minimal AR-DiT TTS training and inference pipeline** that can train on an 8000-hour dataset using a single RTX 5090 (32GB) and produce intelligible speech synthesis results within two days.

> **PS**: The diffusion backbone uses [RFWave](https://github.com/bfs18/rfwave)'s ConvNeXt architecture, not DiT.

## ğŸŒŸ Why Choose This Project?

- ğŸš€ **Resource-Friendly**: Single RTX 5090 (32GB) can handle 8000-hour dataset training
- ğŸ“¦ **Minimal Implementation**: Clean and concise code, easy to understand and modify, suitable for learning and development
- ğŸ‡¨ğŸ‡³ **Chinese-Friendly**: Complete Chinese documentation and Chinese data processing pipeline
- ğŸ¤— **Ready to Use**: Provides pre-trained models and processed datasets for quick start
- ğŸ’¡ **Practical-Oriented**: Achieves intelligible results in two days, better quality with longer training - practical rather than perfect

## ğŸµ Generation Examples

Audio samples generated by the trained model:

<audio controls>
  <source src="outputs/inference_audio_203019_796b492db63e5ccaad85.wav" type="audio/wav">
  Your browser does not support the audio element. <a href="outputs/inference_audio_203019_796b492db63e5ccaad85.wav">Download audio</a>
</audio>

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### ğŸ¤ Inference with Pre-trained Model

We provide pre-trained models on Hugging Face that you can use directly:

**Download Pre-trained Model**:
```bash
huggingface-cli download laupeng1989/armel-checkpoint --local-dir ./models/armel-checkpoint
```

**Run Inference**:
```bash
python3 scripts/mel_inference.py \
  --model_path ./models/armel-checkpoint/ \
  --text example_data/transcript/fanren_short.txt \
  --ref_audio fanren08 \
  --output_path output/generated \
  --dtype bfloat16
```

**Output Files**:
- `output/generated.wav`: Generated audio
- `output/generated.png`: Mel spectrogram visualization
- `output/generated.npy`: Mel spectrogram array

#### ğŸ§ Reference Audio Instructions

The `--ref_audio` parameter specifies the reference audio name (without extension). The script will read the corresponding `.wav` and `.txt` files from the `example_data/voice_prompts/` directory:

```
example_data/voice_prompts/
â”œâ”€â”€ fanren08.wav          # Reference audio
â”œâ”€â”€ fanren08.txt          # Text corresponding to reference audio
â”œâ”€â”€ fanren09.wav
â””â”€â”€ fanren09.txt
```

You can add your own reference audio by placing the audio file and corresponding text file in this directory.

---

## ğŸ”¥ Training from Scratch

If you want to train your own model from scratch, follow these steps.

### ğŸ¤— Training Dataset

We provide a processed training dataset on Hugging Face:

- **Training Dataset**: [laupeng1989/armel-dataset](https://huggingface.co/datasets/laupeng1989/armel-dataset)

**Download Dataset**:
```bash
huggingface-cli download laupeng1989/armel-dataset --repo-type dataset --local-dir ./data/armel-dataset
```

**ğŸ’¡ Tip**: If using the Hugging Face dataset, you can skip the "Data Preparation" section below and proceed directly to training.

### ğŸ“Š Data Preparation

#### 1ï¸âƒ£ Prepare Raw Data

This project uses the [Amphion Emilia preprocessor](https://github.com/open-mmlab/Amphion/tree/main/preprocessors/Emilia) to process raw audio data.

Processed data format:
```
example_data/
â”œâ”€â”€ ä»™é€† ç¬¬87é›† èº«ä¸–è‹é†’ï¼ˆä¸‹ï¼‰ [638031163].json
â”œâ”€â”€ ä»™é€† ç¬¬87é›† èº«ä¸–è‹é†’ï¼ˆä¸‹ï¼‰ [638031163]_000000.m4a
â”œâ”€â”€ ä»™é€† ç¬¬87é›† èº«ä¸–è‹é†’ï¼ˆä¸‹ï¼‰ [638031163]_000001.m4a
â”œâ”€â”€ ä»™é€† ç¬¬87é›† èº«ä¸–è‹é†’ï¼ˆä¸‹ï¼‰ [638031163]_000002.m4a
â””â”€â”€ ...
```

JSON file format (contains segmentation info and text):
```json
[
  {
    "duration": 10.94,
    "text": "[SPEAKER_00] æ¬¢è¿æ”¶å¬...",
    "speaker": 0,
    "parts": [
      {
        "text": "[SPEAKER_00] æ¬¢è¿æ”¶å¬...",
        "start": 4.5125,
        "end": 10.1525,
        "speaker": 0,
        "language": "zh"
      }
    ]
  }
]
```

#### 2ï¸âƒ£ Build Training Dataset

Use `build_dataset.py` to convert raw data to training format:

```bash
python scripts/build_dataset.py \
  --data_dir <your_raw_data_dir> \
  --output_dir <your_output_dir> \
  --num_proc 8 \
  --test_samples 100 \
  --random_seed 42
```

### ğŸ”¥ Training

#### ğŸ’» Training Hardware

This project was trained on **NVIDIA RTX 5090 (32GB)**.

#### âš¡ Training Command

**Prepare Qwen3 Model**:

`model.llm_model_path` can be:
- **Local path**: e.g., `./Qwen3-0.6B` (requires prior download)
- **Hugging Face model name**: e.g., `Qwen/Qwen3-0.6B` (auto-downloads, but first training will be slower)

Recommended to download locally first:
```bash
huggingface-cli download Qwen/Qwen3-0.6B --local-dir ./Qwen3-0.6B
```

**Training Command**:

```bash
python3 scripts/mel_train.py \
  dataset.train_dataset_path=<your_train_data_path> \
  dataset.valid_dataset_path=<your_valid_data_path> \
  model.llm_model_path=./Qwen3-0.6B \
  model.rfmel.batch_mul=2 \
  training.batch_size=4 \
  dataset.max_tokens=1024 \
  training.num_workers=16 \
  training.learning_rate=0.0001 \
  training.log_dir=<your_log_dir> \
  training.diffusion_extra_steps=4 \
  training.check_val_every_n_epoch=1 \
  model.use_skip_connection=true \
  model.estimator.hidden_dim=512 \
  model.estimator.intermediate_dim=1536 \
  model.estimator.num_layers=8
```

**Note**: Lightning automatically detects and uses all available GPUs with DDP strategy. You may need to adjust `batch_size`, `batch_mul`, `max_tokens` based on your hardware configuration.

### ğŸ“¤ Export Model

After training, export the model for inference:

```bash
python scripts/mel_export_checkpoint.py \
  --ckpt_path <your_checkpoint_path>/last.ckpt \
  --output_path ./exported_model/
```

Or specify the checkpoints directory directly (automatically selects the latest):
```bash
python scripts/mel_export_checkpoint.py \
  --ckpt_path <your_checkpoint_dir>/ \
  --output_path ./exported_model/
```

This will generate:
- `model.ckpt`: Model weights
- `model.yaml`: Inference configuration

After exporting, you can use the inference commands from the "Inference with Pre-trained Model" section above.

---

## ğŸ“ Project Structure

```
ar-dit-mel/
â”œâ”€â”€ ar/                      # Autoregressive model
â”‚   â”œâ”€â”€ armel.py            # ARMel main model
â”‚   â”œâ”€â”€ qwen.py             # Qwen3 LLM
â”‚   â””â”€â”€ mel_generate.py     # Mel generation
â”œâ”€â”€ rfwave/                  # Diffusion model
â”‚   â”œâ”€â”€ mel_model.py        # RFMel model
â”‚   â”œâ”€â”€ mel_processor.py    # Mel processor
â”‚   â””â”€â”€ estimator.py        # Diffusion estimator
â”œâ”€â”€ dataset/                 # Dataset
â”œâ”€â”€ scripts/                 # Training and inference scripts
â”‚   â”œâ”€â”€ build_dataset.py    # Build dataset
â”‚   â”œâ”€â”€ mel_train.py        # Training script
â”‚   â”œâ”€â”€ mel_export_checkpoint.py  # Export model
â”‚   â””â”€â”€ mel_inference.py    # Inference script
â””â”€â”€ configs/                 # Configuration files
```

## ğŸ“œ License

MIT License

## ğŸ“š Related Papers

- **Autoregressive Diffusion Transformer for Text-to-Speech Synthesis**
  Zhijun Liu, et al.
  [arXiv:2406.05551](https://arxiv.org/abs/2406.05551)

- **VibeVoice Technical Report**
  Zhiliang Peng, et al.
  [arXiv:2508.19205](https://arxiv.org/abs/2508.19205)

- **VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning**
  Yixuan Zhou, et al.
  [arXiv:2509.24650](https://arxiv.org/abs/2509.24650)

## ğŸ™ Acknowledgments

This project is based on the following open-source projects:
- [Qwen3](https://github.com/QwenLM/Qwen) - Language Model ğŸ¤–
- [Amphion](https://github.com/open-mmlab/Amphion) - Data Preprocessing ğŸµ
- [Vocos](https://github.com/gemelo-ai/vocos) - Vocoder ğŸ”Š
- [RFWave](https://github.com/bfs18/rfwave) - Diffusion Backbone ğŸŒŠ
- [VoxCPM](https://github.com/OpenBMB/VoxCPM) - Architecture Reference ğŸ’¡
- [Higgs-Audio](https://github.com/boson-ai/higgs-audio) - Data Template ğŸ“‹
